"""DeepSeek AI client with RAG integration."""

from openai import OpenAI
import os
from typing import List, Dict, Optional


class DeepSeekClient:
    """Client for DeepSeek API with RAG support."""

    def __init__(self, api_key: str = None, base_url: str = "https://api.deepseek.com"):
        """
        Initialize DeepSeek client.

        Args:
            api_key: DeepSeek API key (or use DEEPSEEK_API_KEY env var)
            base_url: API base URL
        """
        self.api_key = api_key or os.getenv('DEEPSEEK_API_KEY')

        if not self.api_key:
            raise ValueError("DeepSeek API key not provided. Set DEEPSEEK_API_KEY env var or pass api_key parameter.")

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=base_url
        )

        print(f"‚úÖ DeepSeek client initialized")

    def generate_response(
        self,
        user_message: str,
        system_prompt: str = None,
        conversation_history: List[Dict] = None,
        rag_context: List[Dict] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> Dict:
        """
        Generate AI response with optional RAG context.

        Args:
            user_message: User's message
            system_prompt: System prompt (personality definition)
            conversation_history: Previous messages
            rag_context: Retrieved context from RAG
            temperature: Sampling temperature
            max_tokens: Max response tokens

        Returns:
            Dict with response and metadata
        """
        # Build system prompt with RAG context
        final_system_prompt = system_prompt or "–¢—ã –º—É–¥—Ä—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫."

        if rag_context:
            rag_text = self._format_rag_context(rag_context)
            final_system_prompt += f"\n\nüìö –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ö–ù–ò–ì–ò:\n{rag_text}"

        # Build messages
        messages = [
            {"role": "system", "content": final_system_prompt}
        ]

        # Add conversation history
        if conversation_history:
            messages.extend(conversation_history[-10:])  # Last 10 messages

        # Add current user message
        messages.append({"role": "user", "content": user_message})

        # Make API call
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            answer = response.choices[0].message.content
            tokens_used = response.usage.total_tokens

            return {
                'answer': answer,
                'tokens_used': tokens_used,
                'rag_used': bool(rag_context),
                'rag_chunks': len(rag_context) if rag_context else 0
            }

        except Exception as e:
            print(f"‚ùå DeepSeek API error: {e}")
            return {
                'answer': f"–ò–∑–≤–∏–Ω–∏, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}",
                'tokens_used': 0,
                'rag_used': False,
                'error': str(e)
            }

    def generate_with_rag(
        self,
        user_message: str,
        search_engine,
        embedder,
        system_prompt: str = None,
        conversation_history: List[Dict] = None,
        n_results: int = 3,
        temperature: float = 0.7
    ) -> Dict:
        """
        Generate response with automatic RAG retrieval.

        Args:
            user_message: User's message
            search_engine: RAGSearchEngine instance
            embedder: EmbeddingGenerator instance
            system_prompt: Personality prompt
            conversation_history: Previous messages
            n_results: Number of RAG chunks to retrieve
            temperature: Sampling temperature

        Returns:
            Dict with response and metadata
        """
        # Retrieve relevant context from RAG
        print(f"üîç Searching RAG for: '{user_message[:50]}...'")

        rag_results = search_engine.search_with_embedder(
            query=user_message,
            embedder=embedder,
            n_results=n_results
        )

        print(f"‚úÖ Found {len(rag_results)} relevant chunks")

        # Generate response with context
        return self.generate_response(
            user_message=user_message,
            system_prompt=system_prompt,
            conversation_history=conversation_history,
            rag_context=rag_results,
            temperature=temperature
        )

    def _format_rag_context(self, rag_results: List[Dict]) -> str:
        """
        Format RAG results for prompt.

        Args:
            rag_results: List of retrieved chunks

        Returns:
            Formatted context string
        """
        context_parts = []

        for i, result in enumerate(rag_results, 1):
            chapter = result['metadata'].get('chapter_title', 'Unknown')
            text = result['text']

            context_parts.append(f"[–û—Ç—Ä—ã–≤–æ–∫ {i} –∏–∑ –≥–ª–∞–≤—ã '{chapter}']:\n{text}")

        return "\n\n".join(context_parts)


# Personality prompts
JAMES_CLEAR_PROMPT = """
–¢—ã –î–∂–µ–π–º—Å –ö–ª–∏—Ä - –∞–≤—Ç–æ—Ä –±–µ—Å—Ç—Å–µ–ª–ª–µ—Ä–∞ "–ê—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏" (Atomic Habits).

–¢–≤–æ—è —Ñ–∏–ª–æ—Å–æ—Ñ–∏—è:
- –ú–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è ‚Üí –æ–≥—Ä–æ–º–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (1% —É–ª—É—á—à–µ–Ω–∏–π –∫–∞–∂–¥—ã–π –¥–µ–Ω—å)
- –°–∏—Å—Ç–µ–º—ã –≤–∞–∂–Ω–µ–µ —Ü–µ–ª–µ–π
- –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
- 4 –∑–∞–∫–æ–Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è: —Å–¥–µ–ª–∞—Ç—å –æ—á–µ–≤–∏–¥–Ω—ã–º, –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–º, –ª–µ–≥–∫–∏–º, –ø—Ä–∏–Ω–æ—Å—è—â–∏–º —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ

–¢–≤–æ–π —Å—Ç–∏–ª—å:
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –±–µ–∑ –≤–æ–¥—ã
- –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –∏—Å—Ç–æ—Ä–∏–∏
- –ü—Ä–æ—Å—Ç—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏ —Å–∏—Å—Ç–µ–º—ã
- –§–æ–∫—É—Å –Ω–∞ –¥–µ–π—Å—Ç–≤–∏–∏, –∞ –Ω–µ —Ç–µ–æ—Ä–∏–∏

–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:
- Habit stacking (–Ω–∞—Å–ª–æ–µ–Ω–∏–µ –ø—Ä–∏–≤—ã—á–µ–∫)
- 2-minute rule (–ø—Ä–∞–≤–∏–ª–æ 2 –º–∏–Ω—É—Ç)
- Environment design (–¥–∏–∑–∞–π–Ω –æ–∫—Ä—É–∂–µ–Ω–∏—è)
- Identity-based habits (–ø—Ä–∏–≤—ã—á–∫–∏ –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏)

–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-4 –∞–±–∑–∞—Ü–∞), –¥–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏, –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã.
"""


if __name__ == "__main__":
    # Test client
    from dotenv import load_dotenv
    load_dotenv()

    print("üß™ Testing DeepSeek Client\n")

    # Initialize client
    client = DeepSeekClient()

    # Test simple generation
    test_message = "–ö–∞–∫ –º–Ω–µ –Ω–∞—á–∞—Ç—å –±–µ–≥–∞—Ç—å –ø–æ —É—Ç—Ä–∞–º? –Ø –≤—Å–µ–≥–¥–∞ –æ—Ç–∫–ª–∞–¥—ã–≤–∞—é."

    print(f"üí¨ User: {test_message}\n")

    response = client.generate_response(
        user_message=test_message,
        system_prompt=JAMES_CLEAR_PROMPT
    )

    print(f"ü§ñ James Clear:\n{response['answer']}\n")
    print(f"üìä Stats:")
    print(f"   Tokens used: {response['tokens_used']}")
    print(f"   RAG used: {response['rag_used']}")
